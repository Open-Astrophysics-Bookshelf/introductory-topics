{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to a dataset\n",
    "\n",
    "## Introduction\n",
    "Suppose we are given a set of data pairs, $\\{(x_i,y_i)\\}_{i=1,\\ldots,N}$. Let's assume that the $x_i$ are known, but that the $y_i$ have uncertainties. That is, the measured value $y_i$ is within some range of the \"true\".  We further assume (perhaps we have some model or hypothesis) that the $y$ and $x$ are linearly related, although we don't know the slope or intercept of that line.\n",
    "\n",
    "Because each of the $y_i$ has some range of uncertainty, no line will perfectly fit the data, but we can find a best fit line $y(x) = ax + b$ by adjusting the parameters $a$ and $b$ to maximize the probability of our dataset.  In practice this means finding $a$, $b$ that minimize (see chapter 8 of Taylor for details)\n",
    "\\begin{equation*}\n",
    "    \\chi^2 = \\sum_{i=1}^N \\frac{\\left[y_i - y(x_i)\\right]^2}{\\sigma^2}.\n",
    "\\end{equation*}\n",
    "Even though this line will be the one that fits the data the best, it doesn't necessarily imply that the fit is any good.  To determine the goodness-of-fit, we look at $\\chi^2$ and compare its value to what we would expect if our assumptions are correct: namely, that the $y$ and $x$ pairs are linearly related, but the $y$ have some fluctuations that obey a gaussian relation with standard deviation $\\sigma$.\n",
    "\n",
    "To show how this works, we'll use `Python` to construct datasets that violate these assumputions, and then we'll see what happens to the $\\chi^2$ values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the workspace\n",
    "As before, we'll load in `numpy` and `matplotlib.pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a custom `Python` class to make the fake datasets.  `Shift-click` in the following cell to load the class `sampleDataSets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load sample_datasets.py\n",
    "################################################################################\n",
    "# Edward Brown\n",
    "# Michigan State University\n",
    "#\n",
    "# Generates three data sets for a linear relation, one of which fits the data\n",
    "# nearly perfectly and is therefore too good to be true; the other has \n",
    "# \"outliers\" -- points 10 standard deviations from the relation.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "from numpy import linspace,zeros\n",
    "from numpy.random import standard_normal, random, randint\n",
    "\n",
    "class sampleDataSets:\n",
    "    \"\"\"\n",
    "    Sets up a linear relation, y = m*x + b.  Datasets can be generated \n",
    "    from this relation by adding gaussian fluctuations to each y.  The std. \n",
    "    deviation of the fluctuation are chosen from a uniform random distribution \n",
    "    between 0.3 and 0.7.  There are 3 choices for datasets.\n",
    "        1.  Fits the data much better than would be indicated by the size of its\n",
    "            quoted uncertainties. The real fluctations have a std. dev. that is \n",
    "            1/5 of the quoted one.\n",
    "        2.  Uncertainties are drawn from a normal distribution with a standard\n",
    "            deviation matching the size of the errorbars.  This should produce\n",
    "            an ideal chi^2 distribution if many trials are conducted.\n",
    "        3.  Identical to 2, but 20% of the datapoints are given 5 sigma \n",
    "            fluctuations.\n",
    "    \"\"\"\n",
    "    \n",
    "    _slope = 3.0\n",
    "    _intercept = 1.0\n",
    "    _sig_low = 0.3\n",
    "    _sig_high = 0.7\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Sets the relation, the nominal size of the errorbars, and the actual \n",
    "        size of the errorbars.\n",
    "        \"\"\"\n",
    "        a = self._sig_low\n",
    "        b = self._sig_high\n",
    "        self.quoted_error = (b-a)*random() + a\n",
    "        self.fake_error = 0.2*self.quoted_error\n",
    "    \n",
    "    def make_dataset(self,x,use_fake=False,with_outliers=False):\n",
    "        \"\"\"\n",
    "        Constructs a dataset from the given relation with errorbars drawn from \n",
    "        a normal distribution.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            x   :=  [array-like] the values at which the relation should be \n",
    "                    evaluated\n",
    "            use_fake    :=  if True, then reduce the standard deviation of the \n",
    "                            fluctuations by a factor of 10.  This dataset will \n",
    "                            have an anomalously low chi^2.\n",
    "            with_outliers:= if True, then 20% of the points will have 10 sigma \n",
    "                            fluctuations.\n",
    "                            \n",
    "        Returns\n",
    "        -------\n",
    "            y   := an ndarray of length(x.size) containing the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        m = self._slope\n",
    "        b = self._intercept\n",
    "        sig = self.fake_error if use_fake else self.quoted_error\n",
    "        y = m*x + b + sig*standard_normal(len(x))\n",
    "        if with_outliers:\n",
    "            n = int(0.2*x.size)\n",
    "            sgn = np.where(random(n) < 0.5,-1.0,1.0)\n",
    "            indcs = randint(0,x.size-1,size=(2))\n",
    "            y[indcs] = m*x[indcs] + b + sgn*5.0*sig\n",
    "        return y\n",
    "    \n",
    "    @property\n",
    "    def quoted_error(self):\n",
    "        \"\"\"\n",
    "        returns the quoted standard deviation\n",
    "        \"\"\"\n",
    "        return self._amp\n",
    "    \n",
    "    @quoted_error.setter\n",
    "    def quoted_error(self,value):\n",
    "        self._amp = value\n",
    "        \n",
    "    @property\n",
    "    def fake_error(self):\n",
    "        \"\"\"\n",
    "        returns a fake error\n",
    "        \"\"\"\n",
    "        return self._fake\n",
    "    \n",
    "    @fake_error.setter\n",
    "    def fake_error(self,value):\n",
    "        self._fake = value\n",
    "    \n",
    "    def unrealistic_dataset(self,x):\n",
    "        \"\"\"\n",
    "        Returns a dataset with actual uncertainties much less than the quoted \n",
    "        errorbars.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            x   :=  [array-like] the values at which the relation should be \n",
    "                    evaluated\n",
    "                            \n",
    "        Returns\n",
    "        -------\n",
    "            y   := an ndarray of length(x.size) containing the dataset        \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.make_dataset(x,use_fake=True)\n",
    "\n",
    "    def realistic_dataset(self,x):\n",
    "        \"\"\"\n",
    "        Returns a dataset with uncertainties that agree with the quoted \n",
    "        errorbars.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            x   :=  [array-like] the values at which the relation should be \n",
    "                    evaluated\n",
    "                            \n",
    "        Returns\n",
    "        -------\n",
    "            y   := an ndarray of length(x.size) containing the dataset        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.make_dataset(x,use_fake=False)\n",
    "\n",
    "    def dataset_with_outliers(self,x):\n",
    "        \"\"\"\n",
    "        Returns a dataset with 80% of the points drawn from the normal \n",
    "        distribution, and 20% of the points having 10-sigma fluctuations.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "            x   :=  [array-like] the values at which the relation should be \n",
    "                    evaluated\n",
    "                            \n",
    "        Returns\n",
    "        -------\n",
    "            y   := an ndarray of length(x.size) containing the dataset        \n",
    "        \"\"\"\n",
    "\n",
    "        return self.make_dataset(x,use_fake=False,with_outliers=True)\n",
    "        \n",
    "    def true_relation(self,x):\n",
    "        \"\"\"\n",
    "        Returns the true relation, y = m*x + b\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "            x   :=  [array-like] the values at which the relation should be \n",
    "                    evaluated\n",
    "                            \n",
    "        Returns\n",
    "        -------\n",
    "            y   :=  an ndarray of length(x.size) containing the underlying \n",
    "                    relation.\n",
    "        \"\"\"\n",
    "        \n",
    "        m = self._slope\n",
    "        b = self._intercept\n",
    "        return m*x + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some sample datasets\n",
    "Now we are ready to make some trial datasets.  Each dataset will consist of 10 points, `Ndata = 10`.  The line\n",
    "```python\n",
    "d = sampleDataSets()\n",
    "```\n",
    "initializes our sample: it sets the slope, intercept, and standard deviation of our points.  We then set some trial $x$-values in the range $[0.0,3.0]$\n",
    "```python\n",
    "x = np.linspace(0.0,3.0,Ndata)\n",
    "```\n",
    "calculate the true linear realation\n",
    "```python\n",
    "t = d.true_relation(xfit)\n",
    "```\n",
    "and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ndata = 10\n",
    "d = sampleDataSets()\n",
    "x = np.linspace(0.0,3.0,Ndata)\n",
    "t = d.true_relation(x)\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('t')\n",
    "plt.title('The actual relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how some datasets might look.  We'll pick some $x$-values in the interval $[0,3)$ at random,\n",
    "\n",
    "    x = 3.0*random(Ndata)\n",
    "    \n",
    "and generate three datasets: one with a realistic distribution of $y$ values, given the uncertainties; one with an unrealistic distribution of $y$ values; and one with two \"outlier\" points.  You can rerun this cell several times to get a different realization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = d.realistic_dataset(x)\n",
    "f = d.unrealistic_dataset(x)\n",
    "o = d.dataset_with_outliers(x)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('A realistic dataset')\n",
    "plt.xlim(-0.3,3.3)\n",
    "plt.errorbar(x,r,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('An unrealistic dataset')\n",
    "plt.xlim(-0.3,3.3)\n",
    "plt.errorbar(x,f,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('A dataset with outliers')\n",
    "plt.xlim(-0.3,3.3)\n",
    "plt.errorbar(x,o,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Explain why the middle dataset is unrealistic. **Double-click in this box and type your answer below.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data\n",
    "\n",
    "The numpy routine `polyfit(x,s,1)` will attempt to fit a polynomial of degree 1 (that is, a linear function $y=ax+b$) to the dataset `s`.  It does this by finding the values of $a$ and $b$ that minimize $\\chi^2$. Let's try it with our 3 datasets.  As before, the blue line is the true relation; the black dashed line is the one with a minimum $\\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr = np.polyfit(x,r,1)\n",
    "yr = pr[0]*x + pr[1]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.errorbar(x,r,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,yr,color='black',linestyle='dashed')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('A realistic dataset')\n",
    "\n",
    "pf = np.polyfit(x,f,1)\n",
    "yf = pf[0]*x + pf[1]\n",
    "plt.subplot(1,3,2)\n",
    "plt.errorbar(x,f,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,yf,color='black',linestyle='dashed')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('An unrealistic dataset')\n",
    "\n",
    "po = np.polyfit(x,o,1)\n",
    "yo = po[0]*x + po[1]\n",
    "plt.subplot(1,3,3)\n",
    "plt.errorbar(x,o,yerr=d.quoted_error,fmt='r.')\n",
    "plt.plot(x,yo,color='black',linestyle='dashed')\n",
    "plt.plot(x,t)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('A dataset with outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these data sets, how does the fitted line compare with the actual relation?\n",
    "\n",
    "**Type your response in this cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data\n",
    "Now we'll look at the $\\chi^2$ test for the data, where $\\chi^2$ is defined in the introduction, above. First, we'll define a function `chi2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chi2(y,d,s):\n",
    "    \"\"\"\n",
    "    Computes chi^2 for a set of fitted points and data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        y := [array] points y(x) from the fit\n",
    "        d := [array] data points d(x)\n",
    "        s := std.deviation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        chi2\n",
    "    \"\"\"\n",
    "    \n",
    "    q2 = (y-d)**2 / s**2\n",
    "    return np.sum(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it to the test.  We'll make `Ntrials = 1000` datasets.  We'll compute $\\chi^2$ in two ways.  The first, using our \"true\" values for $a$ and $b$; the second, using an values of $a$ and $b$ that are best fit from the data.  \n",
    "\n",
    "**Stop:** Before proceeding, *predict* how the values of $\\chi^2$ computed with these two methods will differ. Do you think they will give the same values?  If not, which one will give a smaller value?\n",
    "\n",
    "### The $\\chi^2$ distribution\n",
    "First, we'll allocate two arrays, each of length `Ntrials`, to hold our results.\n",
    "\n",
    "```python\n",
    "chi2fit = np.zeros(Ntrials)\n",
    "chi2best = np.zeros(Ntrials)\n",
    "```\n",
    "\n",
    "Next, we'll make an array `t` of the \"true\" values of `y` for each `x`.\n",
    "\n",
    "```python\n",
    "t = d.true_relation(x)\n",
    "```\n",
    "\n",
    "Now we will run our trials.  For each trial, we'll make a realstic dataset, `s`, and then fit a line to that data and compute our estimated values of `y` for each `x`.\n",
    "\n",
    "```python\n",
    "p = np.polyfit(x,s,1)\n",
    "y = x*p[0]+p[1]\n",
    "```\n",
    "\n",
    "Finally, we'll compute the $\\chi^2$ for our \"true\" relation and for the fitted $a,b$.\n",
    "\n",
    "```python\n",
    "chi2fit[n] = chi2(t,s,d.quoted_error)\n",
    "chi2best[n] = chi2(y,s,d.quoted_error)\n",
    "```\n",
    "\n",
    "We'll then plot the histograms of the two different $\\chi^2$ distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ntrials = 1000\n",
    "chi2fit = np.zeros(Ntrials)\n",
    "chi2best = np.zeros(Ntrials)\n",
    "# t contains the true y-values\n",
    "t = d.true_relation(x)\n",
    "\n",
    "for n in range(Ntrials):\n",
    "    # for each trial make a realistic dataset\n",
    "    s = d.realistic_dataset(x)\n",
    "    # fit it, and get the y-values for that fit\n",
    "    p = np.polyfit(x,s,1)\n",
    "    y = x*p[0]+p[1]\n",
    "    # Now compute the chi-square value comparing the data points (s) against \n",
    "    # the true relation t and the fitted relation y\n",
    "    chi2fit[n] = chi2(t,s,d.quoted_error)\n",
    "    chi2best[n] = chi2(y,s,d.quoted_error)\n",
    "alabel = '{0:5.2f}'.format(np.average(chi2fit))\n",
    "flabel = '{0:5.2f}'.format(np.average(chi2best))\n",
    "plt.xlabel(r'$\\chi^2$')\n",
    "plt.ylabel(r'$N$')\n",
    "plt.hist(chi2fit,bins=20,align='mid',histtype='step',label=r'actual: $\\langle\\chi^2\\rangle = '+alabel+r'$')\n",
    "plt.hist(chi2best,bins=20,align='mid',histtype='step',label=r'best-fit: $\\langle\\chi^2\\rangle = '+flabel+r'$')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Explain the difference between these two distributions of $\\chi^2$. Are the averages what you expect? **Double-click in this box and type your answer below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** It is common to define a *reduced chi-square*, $\\tilde{\\chi}^2 = \\chi^2/d$, where $d = N - c$ is the *degree of freedom* with $N$ the number of data points and $c$ the number of adjustable parameters in the fit.  See section 12.3 of Taylor for further discussion.  For the plot above, what is $\\tilde{\\chi}^2$ for the best-fit case? Explain why $\\tilde{\\chi}^2$ is a useful quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our datasets\n",
    "Now that we have seen the distribution of $\\chi^2$ for our \"realistic\" dataset, let's look at how we can use this test to assess whether our fit is plausible.\n",
    "\n",
    "Recall that we stored our datasets in variables `r` (realistic), `f` (fake), and `o` (with outliers).  We'll now fit each of those in turn, and compute a $\\chi^2$ for each, which we'll store in the variables `chi2r`, `chi2f`, `chi2o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = np.polyfit(x,r,1)\n",
    "y = x*p[0]+p[1]\n",
    "chi2r = chi2(y,r,d.quoted_error)\n",
    "print('chi-squared for realistic data = {0:5.2f}'.format(chi2r))\n",
    "\n",
    "p = np.polyfit(x,f,1)\n",
    "y = x*p[0]+p[1]\n",
    "chi2f = chi2(y,f,d.quoted_error)\n",
    "print('chi-squared for data with over-large error bars = {0:5.2f}'.format(chi2f))\n",
    "\n",
    "p = np.polyfit(x,o,1)\n",
    "y = x*p[0]+p[1]\n",
    "chi2o = chi2(y,o,d.quoted_error)\n",
    "print('chi-squared for data with outliers = {0:5.2f}'.format(chi2o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at where these values of $\\chi^2$ lie on our distribution `chi2best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(r'$\\chi^2$ for a dataset that matches hypothesis')\n",
    "plt.xlabel(r'$\\chi^2$')\n",
    "plt.ylabel(r'$N$')\n",
    "hst,bn,ptch = plt.hist(chi2best,bins=20,align='mid',histtype='step', label='dist. best-fit')\n",
    "plt.vlines(chi2r, 0.0, max(hst), colors='k', linestyles='solid',label='matches hypothesis')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** For this case, what is the probability that a dataset with random uncertainties would have a worse $\\chi^2$?  Is it closest to 0, 0.5, or 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(r'$\\chi^2$ for a dataset with unrealistic errorbars')\n",
    "plt.xlabel(r'$\\chi^2$')\n",
    "plt.ylabel(r'$N$')\n",
    "hst,bn,ptch = plt.hist(chi2best,bins=20,align='mid',histtype='step', label='best-fit dist.')\n",
    "plt.vlines(chi2f,0.0,max(hst),colors = 'g', linestyles='dashed', label='too large errorbars')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** For this case, what is the probability that a dataset with random uncertainties would have a worse $\\chi^2$?  Is it closest to 0, 0.5, or 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(r'$\\chi^2$ for a dataset with outliers')\n",
    "plt.xlabel(r'$\\chi^2$')\n",
    "plt.ylabel(r'$N$')\n",
    "hst,bn,ptch = plt.hist(chi2best,bins=20,align='mid',histtype='step', label='best-fit dist.')\n",
    "plt.vlines(chi2o,0.0,max(hst),colors = 'r', linestyles='dotted', label='with outliers')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** For this case, what is the probability that a dataset with random uncertainties would have a worse $\\chi^2$?  Is it closest to 0, 0.5, or 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\chi^2$ test\n",
    "There is a theoretical distribution of $\\chi^2$ values.  To see whether our fit is reasonable, we compute its value of $\\chi^2$ and compare that value against the distribution.  Our question is, \"If the dataset is as we expect, what is the probability of getting a value of $\\chi^2$ larger than ours?\"  If this is very unlikely, we conclude that our fit does not describe the data.\n",
    "\n",
    "Alternatively, we may get a probability that is very close to 1; in other words, almost *any* dataset would have a worse fit.  This may suggest that the error bars are overestimated; alternatively, they could indicated the data has been faked or \"fudged\" to line up with the hypothesis.\n",
    "\n",
    "The following block of code loads the `scipy.stats` module; it then defines a $\\chi^2$ distribution with 8 degrees of freedom (10 data points - 2 adjustable parameters), and calculates the cumulative probability of getting a value of $\\chi^2$ *larger* than `chi2r`, `chi2f`, and `chi2o`.  Then to make things interesting, we scramble the order of our probabilities and print them.\n",
    "\n",
    "Your task: decide which probability matches the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "df = 8# there are 10 data points and 2 adjustable parameters, so we have 8 degrees of freedom.\n",
    "c2 = st.chi2(df)\n",
    "ps = (c2.sf(chi2r),c2.sf(chi2f),c2.sf(chi2o))\n",
    "# scramble the probabilities\n",
    "ps = np.random.permutation(ps)\n",
    "for lab, p in zip('abc',ps):\n",
    "    print('{0}. {1:9.2e}'.format(lab,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Edit this cell.  Beside each letter write the value of $p$ and the dataset — realistic, overlarge errorbars, or outliers — to which it corresponds.  Explain your reasoning.\n",
    "\n",
    "a. \n",
    "\n",
    "b. \n",
    "\n",
    "c. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
